OUR PROJECT ---------EMOTISENSE---------

Introduction:

EmotiSense is a groundbreaking multimodal emotion detection system designed to provide a comprehensive understanding of human emotions in digital interactions. Leveraging the power of Large Language Models (LLMs) such as BERT, Wav2vec, and LLaVa, EmotiSense integrates textual, auditory, and visual data modalities to transcend the limitations of traditional unimodal approaches.

My Contributions:

Audio Analysis: spearheaded the audio analysis component, ensuring accurate emotion detection by leveraging Wav2vec for processing speech patterns and tonality.
FastAPI Backend: engineered the FastAPI backend for seamless data processing, enabling efficient communication between the frontend and the data processing modules.
Streamlit Integration: facilitated integration with Streamlit for user-friendly visualization and interaction, enhancing the accessibility and usability of EmotiSense.

Technologies Used:

BERT: Primarily utilized for textual data analysis, BERT enhances EmotiSense's ability to understand textual cues indicative of emotional states.
Wav2vec: Crucial for audio data analysis, Wav2vec processes speech patterns and tonality to extract auditory cues related to emotions.
LLaVa: Primarily used for visual data analysis and image processing, LLaVa contributes to EmotiSense's ability to analyze facial expressions and visual cues associated with emotions.


Usage:

Textual Analysis: Input text data into the provided interface to analyze textual cues indicative of emotional states using BERT.
Audio Analysis: Upload audio files to the interface to analyze speech patterns and tonality using Wav2vec.
Visual Analysis: Provide image data to analyze facial expressions and visual cues associated with emotions using LLaVa.

